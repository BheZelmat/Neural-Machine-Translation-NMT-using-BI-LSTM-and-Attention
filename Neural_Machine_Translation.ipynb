{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9cb49525",
      "metadata": {
        "id": "9cb49525"
      },
      "source": [
        "#  Neural Machine Translation\n",
        "\n",
        "We will build an English-to-Portuguese neural machine translation (NMT) model using Long Short-Term Memory (LSTM) networks with attention. Machine translation is an important task in natural language processing and could be useful not only for translating one language to another but also for word sense disambiguation (e.g. determining whether the word \"bank\" refers to the financial bank, or the land alongside a river). Implementing this using just a Recurrent Neural Network (RNN) with LSTMs can work for short to medium length sentences but can result in vanishing gradients for very long sequences. To help with this, we will be adding an attention mechanism to allow the decoder to access all relevant parts of the input sentence regardless of its length, we will:\n",
        "\n",
        "- Implement an encoder-decoder system with attention\n",
        "- Build the NMT model from scratch using Tensorflow\n",
        "- Generate translations using greedy and Minimum Bayes Risk (MBR) decoding\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install tensorflow_text\n"
      ],
      "metadata": {
        "id": "8i4aAqtQu0vk"
      },
      "id": "8i4aAqtQu0vk",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f9ef370d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "f9ef370d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Setting this env variable prevents TF warnings from showing up\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pathlib\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e76be1dc",
      "metadata": {
        "id": "e76be1dc"
      },
      "source": [
        "\n",
        "## 1. Data Preparation\n",
        "\n",
        "The steps performed for text pre-processing can be summarized as:\n",
        "\n",
        "- Reading the raw data from the text files\n",
        "- Cleaning the data (using lowercase, adding space around punctuation, trimming whitespaces, etc)\n",
        "- Splitting it into training and validation sets\n",
        "- Adding the start-of-sentence and end-of-sentence tokens to every sentence\n",
        "- Tokenizing the sentences\n",
        "- Creating a Tensorflow dataset out of the tokenized sentences\n",
        "\n",
        "Take a moment to inspect the raw sentences:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    text = path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    lines = text.splitlines()\n",
        "    pairs = [line.split(\"\\t\") for line in lines]\n",
        "\n",
        "    context = np.array([context for target, context, _ in pairs])\n",
        "    target = np.array([target for target, context, _ in pairs])\n",
        "\n",
        "    return context, target\n"
      ],
      "metadata": {
        "id": "lmMT9w6xw5-X"
      },
      "id": "lmMT9w6xw5-X",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = pathlib.Path(\"por.txt\")"
      ],
      "metadata": {
        "id": "4FHd3ExsxBxV"
      },
      "id": "4FHd3ExsxBxV",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portuguese_sentences, english_sentences = load_data(path_to_file)"
      ],
      "metadata": {
        "id": "Gg25ggBKw6Bu"
      },
      "id": "Gg25ggBKw6Bu",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(english_sentences)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "is_train = np.random.uniform(size=(len(portuguese_sentences),)) < 0.8\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (english_sentences[is_train], portuguese_sentences[is_train])\n",
        "    )\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "val_raw = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (english_sentences[~is_train], portuguese_sentences[~is_train])\n",
        "    )\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "EFdgOKapw6Ej"
      },
      "id": "EFdgOKapw6Ej",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tf_lower_and_split_punct(text):\n",
        "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
        "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
        "    return text\n",
        "\n",
        "\n",
        "max_vocab_size = 12000\n",
        "\n",
        "english_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True\n",
        ")\n",
        "\n",
        "english_vectorizer.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "portuguese_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True\n",
        ")\n",
        "\n",
        "portuguese_vectorizer.adapt(train_raw.map(lambda context, target: target))\n",
        "\n"
      ],
      "metadata": {
        "id": "96gX-aafw6ID"
      },
      "id": "96gX-aafw6ID",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_text(context, target):\n",
        "    context = english_vectorizer(context).to_tensor()\n",
        "    target = portuguese_vectorizer(target)\n",
        "    targ_in = target[:, :-1].to_tensor()\n",
        "    targ_out = target[:, 1:].to_tensor()\n",
        "    return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_data = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_data = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "rIpREkA_w6LD"
      },
      "id": "rIpREkA_w6LD",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quick test"
      ],
      "metadata": {
        "id": "rdxbDSQiyazk"
      },
      "id": "rdxbDSQiyazk"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "226033a1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "226033a1",
        "outputId": "21b93d2f-b00d-49f2-b898-70fcb9f25d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English (to translate) sentence:\n",
            "\n",
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n",
            "\n",
            "Portuguese (translation) sentence:\n",
            "\n",
            "Não importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
        "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba90eb9",
      "metadata": {
        "id": "5ba90eb9"
      },
      "source": [
        "We don't have much use for the raw sentences so delete them to save memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d9f081b0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "d9f081b0"
      },
      "outputs": [],
      "source": [
        "del portuguese_sentences\n",
        "del english_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2c1cfc17",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c1cfc17",
        "outputId": "8ede55ed-1497-44ec-b58c-ed9cb26be336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 words of the english vocabulary:\n",
            "\n",
            "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'i', 'to', 'you', 'the']\n",
            "\n",
            "First 10 words of the portuguese vocabulary:\n",
            "\n",
            "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'que', 'o', 'nao', 'eu']\n"
          ]
        }
      ],
      "source": [
        "print(f\"First 10 words of the english vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
        "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{portuguese_vectorizer.get_vocabulary()[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3152b075",
      "metadata": {
        "id": "3152b075"
      },
      "source": [
        "Notice that the first 4 words are reserved for special words. In order, these are:\n",
        "\n",
        "- the empty string\n",
        "- a special token to represent an unknown word\n",
        "- a special token to represent the start of a sentence\n",
        "- a special token to represent the end of a sentence\n",
        "\n",
        "We can see how many words are in a vocabulary by using the `vocabulary_size` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5facaa0c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5facaa0c",
        "outputId": "7b9bd23e-5276-4812-af93-d10801e9155b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portuguese vocabulary is made up of 12000 words\n"
          ]
        }
      ],
      "source": [
        "# Size of the vocabulary\n",
        "vocab_size = portuguese_vectorizer.vocabulary_size()\n",
        "\n",
        "print(f\"Portuguese vocabulary is made up of {vocab_size} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e4b615",
      "metadata": {
        "id": "53e4b615"
      },
      "source": [
        "We can define [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) objects that will help us map from words to ids and vice versa. Do this for the portuguese vocabulary since this will be useful later on when we decode the predictions from Our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "218f7a36",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "218f7a36"
      },
      "outputs": [],
      "source": [
        "# This helps us convert from words to ids\n",
        "word_to_id = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token=\"\",\n",
        "    oov_token=\"[UNK]\"\n",
        ")\n",
        "\n",
        "# This helps us convert from ids to words\n",
        "id_to_word = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token=\"\",\n",
        "    oov_token=\"[UNK]\",\n",
        "    invert=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af8b623",
      "metadata": {
        "id": "4af8b623"
      },
      "source": [
        "Try it out for the special tokens and a random word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "20076b9a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20076b9a",
        "outputId": "87d24a44-0411-4bf9-b163-0a27442956b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The id for the [UNK] token is 1\n",
            "The id for the [SOS] token is 2\n",
            "The id for the [EOS] token is 3\n",
            "The id for baunilha (vanilla) is 7049\n"
          ]
        }
      ],
      "source": [
        "unk_id = word_to_id(\"[UNK]\")\n",
        "sos_id = word_to_id(\"[SOS]\")\n",
        "eos_id = word_to_id(\"[EOS]\")\n",
        "baunilha_id = word_to_id(\"baunilha\")\n",
        "\n",
        "print(f\"The id for the [UNK] token is {unk_id}\")\n",
        "print(f\"The id for the [SOS] token is {sos_id}\")\n",
        "print(f\"The id for the [EOS] token is {eos_id}\")\n",
        "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f1d744c",
      "metadata": {
        "id": "2f1d744c"
      },
      "source": [
        "Finally take a look at how the data that is going to be fed to the neural network looks like. Both `train_data` and `val_data` are of type `tf.data.Dataset` and are already arranged in batches of 64 examples. To get the first batch out of a tf dataset you can use the `take` method. To get the first example out of the batch you can slice the tensor and use the `numpy` method for nicer printing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "739777eb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "739777eb",
        "outputId": "3fb11f8c-ae3a-48ae-e1d7-73c5711af38b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized english sentence:\n",
            "[  2   6 164   8 248 154 294   4   3   0   0   0   0   0   0   0   0   0\n",
            "   0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence (shifted to the right):\n",
            "[   2    9  151    6   14   26 2376  112  431    4    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence:\n",
            "[   9  151    6   14   26 2376  112  431    4    3    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for (to_translate, sr_translation), translation in train_data.take(1):\n",
        "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd41cb52",
      "metadata": {
        "id": "dd41cb52"
      },
      "source": [
        "\n",
        "## 2. NMT model with attention\n",
        "\n",
        "The model we will build uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) takes in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. Using a a regular sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer ones. We can picture it as all of the context of the input sentence is compressed into one vector that is passed into the decoder block. this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.\n",
        "\n",
        "\n",
        "\n",
        "Adding an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence.\n",
        "\n",
        "\n",
        "There are different ways to implement attention and the one we'll use for this Notebook is the Scaled Dot Product Attention which has the form:\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "We can think of it as computing scores using queries (Q) and keys (K), followed by a multiplication of values (V) to get a context vector at a particular timestep of the decoder. This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality ($\\sqrt{d_k}$) is for improving model performance. For our machine translation application, the encoder activations (i.e. encoder hidden states) will be the keys and values, while the decoder activations (i.e. decoder hidden states) will be the queries.\n",
        "\n",
        "We will see in the upcoming sections that this complex architecture and mechanism can be implemented with just a few lines of code.\n",
        "\n",
        "First you will define two important global variables:\n",
        "\n",
        "- The size of the vocabulary\n",
        "- The number of units in the LSTM layers (the same number will be used for all LSTM layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2e484abf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "2e484abf"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 12000\n",
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc251965",
      "metadata": {
        "id": "cc251965"
      },
      "source": [
        "\n",
        "## 1 - Encoder\n",
        "\n",
        "Our first step is to code the encoder part of the neural network. We will create an `Encoder` class.\n",
        "Notice that in the constructor (the `__init__` method) we need to define all of the sublayers of the encoder and then use these sublayers during the forward pass (the `call` method).\n",
        "\n",
        "The encoder consists of the following layers:\n",
        "\n",
        "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). For this layer we need to define the appropriate `input_dim` and `output_dim` and let it know that you are using '0' as padding, which can be done by using the appropriate value for the `mask_zero` parameter.\n",
        "    \n",
        "+ [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). In TF you can implement bidirectional behaviour for RNN-like layers. This part is already taken care of but we will need to specify the appropriate type of layer as well as its parameters. In particular we need to set the appropriate number of units and make sure that the LSTM returns the full sequence and not only the last output, which can be done by using the appropriate value for the `return_sequences` parameter.\n",
        "\n",
        "\n",
        "We need to define the forward pass using the syntax of TF's [functional API](https://www.tensorflow.org/guide/keras/functional_api).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b1db0a1d",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "b1db0a1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=units,\n",
        "            mask_zero=True\n",
        "        )\n",
        "\n",
        "        self.rnn = tf.keras.layers.Bidirectional(\n",
        "            merge_mode=\"sum\",\n",
        "            layer=tf.keras.layers.LSTM(\n",
        "                units=units,\n",
        "                return_sequences=True\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(self, context):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            context (tf.Tensor): The sentence to translate\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Encoded sentence to translate\n",
        "        \"\"\"\n",
        "        # Pass the context through the embedding layer\n",
        "        x = self.embedding(context)\n",
        "\n",
        "        # Pass the output of the embedding through the RNN\n",
        "        x = self.rnn(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "65034ffd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65034ffd",
        "outputId": "5ae39772-8c4a-4cd6-d839-5e03aada662b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of sentences in english has shape: (64, 19)\n",
            "\n",
            "Encoder output has shape: (64, 19, 256)\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of our implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "\n",
        "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
        "\n",
        "# Pass a batch of sentences to translate from english to portuguese\n",
        "encoder_output = encoder(to_translate)\n",
        "\n",
        "print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
        "print(f'Encoder output has shape: {encoder_output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1afe83f4",
      "metadata": {
        "id": "1afe83f4"
      },
      "source": [
        "\n",
        "## 2 - CrossAttention\n",
        "\n",
        "Next step is to code the layer that will perform cross attention between the original sentences and the translations. For this, lets code the `CrossAttention` class below.\n",
        "Notice that in the constructor (the `__init__` method) we need to define all of the sublayers and then use these sublayers during the forward pass (the `call` method).\n",
        "\n",
        "The cross attention consists of the following layers:\n",
        "\n",
        "- [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). For this layer we need to define the appropriate `key_dim`, which is the size of the key and query tensors. We will also need to set the number of heads to 1 since you aren't implementing multi head attention but attention between two tensors. The reason why this layer is preferred over [Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) is that it allows simpler code during the forward pass.\n",
        "    \n",
        "A couple of things to notice:\n",
        "- We need a way to pass both the output of the attention alongside the shifted-to-the-right translation (since this cross attention happens in the decoder side). For this we will use an [Add](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add) layer so that the original dimension is preserved, which would not happen if you use something like a [Concatenate](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate) layer.\n",
        "\n",
        "+ Layer normalization is also performed for better stability of the network by using a [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization) layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "74e71f3d",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "74e71f3d"
      },
      "outputs": [],
      "source": [
        "# GRADED CLASS: CrossAttention\n",
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = (\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                key_dim=units,\n",
        "                num_heads=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, context, target):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            context (tf.Tensor): Encoded sentence to translate\n",
        "            target (tf.Tensor): The embedded shifted-to-the-right translation\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Cross attention between context and target\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # Call the MH attention by passing in the query and value\n",
        "        # For this case the query should be the translation and the value the encoded sentence to translate\n",
        "        # Hint: Check the call arguments of MultiHeadAttention in the docs\n",
        "        attn_output = self.mha(\n",
        "            query=target,\n",
        "            value=context\n",
        "        )\n",
        "\n",
        "\n",
        "        x = self.add([target, attn_output])\n",
        "\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4c62796f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c62796f",
        "outputId": "d3b5f0de-182c-48fa-9a41-f98e843657d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of contexts has shape: (64, 19, 256)\n",
            "Tensor of translations has shape: (64, 16, 256)\n",
            "Tensor of attention scores has shape: (64, 16, 256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'cross_attention_4' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of your implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "# The attention layer expects the embedded sr-translation and the context\n",
        "# The context (encoder_output) is already embedded so you need to do this for sr_translation:\n",
        "sr_translation_embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
        "\n",
        "# Compute the cross attention\n",
        "attention_result = attention_layer(encoder_output, sr_translation_embed)\n",
        "\n",
        "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
        "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
        "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa296ee2",
      "metadata": {
        "id": "aa296ee2"
      },
      "source": [
        "\n",
        "## Exercise 3 - Decoder\n",
        "\n",
        "\n",
        "Now we will implement the decoder part of the neural network completing the `Decoder` class.\n",
        "\n",
        "\n",
        "The decoder consists of the following layers:\n",
        "\n",
        "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). For this layer we need to define the appropriate `input_dim` and `output_dim` and let it know that we are using '0' as padding, which can be done by using the appropriate value for the `mask_zero` parameter.\n",
        "  \n",
        "  \n",
        "+ Pre-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Unlike in the encoder in which you used a Bidirectional LSTM, here we will use a vanilla LSTM. Don't forget to set the appropriate number of units and make sure that the LSTM returns the full sequence and not only the last output, which can be done by using the appropriate value for the `return_sequences` parameter.\n",
        "\n",
        "It is very important that this layer returns the state since this will be needed for inference so make sure to set the `return_state` parameter accordingly. Notice that LSTM layers return state as a tuple of two tensors called `memory_state` and `carry_state`, **however these names have been changed to better reflect what you have seen in the lectures to `hidden_state` and `cell_state` respectively**.\n",
        "\n",
        "- The attention layer that performs cross attention between the sentence to translate and the right-shifted translation. Here you need to use the `CrossAttention` layer you defined in the previous exercise.\n",
        "\n",
        "+ Post-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Another LSTM layer. For this one we don't need it to return the state.\n",
        "\n",
        "- Finally a [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer. This one should have the same number of units as the size of the vocabulary since we expect it to compute the logits for every possible word in the vocabulary. Make sure to use a `logsoftmax` activation function for this one, which you can get as [tf.nn.log_softmax](https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e9639bdb",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "e9639bdb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # The embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=units,\n",
        "            mask_zero=True\n",
        "        )\n",
        "\n",
        "        # The RNN before attention\n",
        "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units=units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "\n",
        "        # The attention layer\n",
        "        self.attention = CrossAttention(units)\n",
        "\n",
        "        # The RNN after attention\n",
        "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units=units,\n",
        "            return_sequences=True\n",
        "        )\n",
        "\n",
        "        # The dense layer with logsoftmax activation\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            units=vocab_size,\n",
        "            activation= tf.nn.log_softmax\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(self, context, target, state=None, return_state=False):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            context (tf.Tensor): Encoded sentence to translate\n",
        "            target (tf.Tensor): The shifted-to-the-right translation\n",
        "            state (list[tf.Tensor, tf.Tensor], optional): Hidden state of the pre-attention LSTM. Defaults to None.\n",
        "            return_state (bool, optional): If set to true return the hidden states of the LSTM. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # Get the embedding of the input\n",
        "        x = self.embedding(target)\n",
        "\n",
        "        # Pass the embedded input into the pre attention LSTM\n",
        "\n",
        "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
        "\n",
        "        # Perform cross attention between the context and the output of the LSTM (in that order)\n",
        "        x = self.attention(context, x)\n",
        "\n",
        "        # Do a pass through the post attention LSTM\n",
        "        x = self.post_attention_rnn(x)\n",
        "\n",
        "        # Compute the logits\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "\n",
        "        if return_state:\n",
        "            return logits, [hidden_state, cell_state]\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f6165cf2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6165cf2",
        "outputId": "1b33524b-820e-4ed9-cfb4-4984b5a568f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'cross_attention_5' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of contexts has shape: (64, 19, 256)\n",
            "Tensor of right-shifted translations has shape: (64, 16)\n",
            "Tensor of logits has shape: (64, 16, 12000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'decoder' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of your implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
        "\n",
        "# Notice that you don't need the embedded version of sr_translation since this is done inside the class\n",
        "logits = decoder(encoder_output, sr_translation)\n",
        "\n",
        "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dcce3a7",
      "metadata": {
        "id": "9dcce3a7"
      },
      "source": [
        "\n",
        "## 4 - Translator\n",
        "\n",
        "Now we have to put together all of the layers you previously coded into an actual model. For this, we'll code the `Translator` class below. Notice how unlike the Encoder and Decoder classes inherited from `tf.keras.layers.Layer`, the Translator class inherits from `tf.keras.layers.Model`.\n",
        "\n",
        "Remember that `train_data` will yield a tuple with the sentence to translate and the shifted-to-the-right translation, which are the \"features\" of the model. This means that the inputs of your network will be tuples containing context and targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "205fcf31",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "205fcf31"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Translator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # Define the encoder with the appropriate vocab_size and number of units\n",
        "        self.encoder = Encoder(vocab_size,units)\n",
        "\n",
        "        # Define the decoder with the appropriate vocab_size and number of units\n",
        "        self.decoder = Decoder(vocab_size,units)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            inputs (tuple(tf.Tensor, tf.Tensor)): Tuple containing the context (sentence to translate) and the target (shifted-to-the-right translation)\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
        "        \"\"\"\n",
        "\n",
        "        # In this case inputs is a tuple consisting of the context and the target, unpack it into single variables\n",
        "        context, target = inputs\n",
        "\n",
        "        # Pass the context through the encoder\n",
        "        encoded_context = self.encoder(context)\n",
        "\n",
        "        # Compute the logits by passing the encoded context and the target to the decoder\n",
        "        logits = self.decoder(encoded_context,target)\n",
        "\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4d4a231c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d4a231c",
        "outputId": "e1c12c6a-f10a-4f0c-a653-ef78de021173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'cross_attention_6' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'decoder_1' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of sentences to translate has shape: (64, 19)\n",
            "Tensor of right-shifted translations has shape: (64, 16)\n",
            "Tensor of logits has shape: (64, 16, 12000)\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of your implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "translator = Translator(VOCAB_SIZE, UNITS)\n",
        "\n",
        "# Compute the logits for every word in the vocabulary\n",
        "logits = translator((to_translate, sr_translation))\n",
        "\n",
        "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f81bc228",
      "metadata": {
        "id": "f81bc228"
      },
      "source": [
        "\n",
        "## 3. Training\n",
        "\n",
        "Now that we have an untrained instance of the NMT model, it is time to train it. But first we need to define a `masked_loss` and a `masked_accuracy` functions, so that the padding doesn't affect the calculations of the loss and accuracy, then we can use the `compile_and_train` function below to achieve this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Check which elements of y_true are padding\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "\n",
        "    loss *= mask\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def masked_acc(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
      ],
      "metadata": {
        "id": "7iBUGFB_5Bvk"
      },
      "id": "7iBUGFB_5Bvk",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "8a61ef65",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "8a61ef65"
      },
      "outputs": [],
      "source": [
        "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
        "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data.repeat(),\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_data,\n",
        "        validation_steps=50,\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
        "    )\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "87d9bf9f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87d9bf9f",
        "outputId": "343acbfc-3156-454f-cd07-13a6cc79f200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 152ms/step - loss: 5.7552 - masked_acc: 0.1618 - masked_loss: 5.7552 - val_loss: 4.3079 - val_masked_acc: 0.3427 - val_masked_loss: 4.3079\n",
            "Epoch 2/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 147ms/step - loss: 3.9851 - masked_acc: 0.3895 - masked_loss: 3.9851 - val_loss: 3.0193 - val_masked_acc: 0.5011 - val_masked_loss: 3.0193\n",
            "Epoch 3/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 150ms/step - loss: 2.8443 - masked_acc: 0.5302 - masked_loss: 2.8443 - val_loss: 2.3734 - val_masked_acc: 0.5917 - val_masked_loss: 2.3734\n",
            "Epoch 4/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 149ms/step - loss: 2.2932 - masked_acc: 0.6048 - masked_loss: 2.2932 - val_loss: 1.9514 - val_masked_acc: 0.6508 - val_masked_loss: 1.9514\n",
            "Epoch 5/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 151ms/step - loss: 1.9554 - masked_acc: 0.6554 - masked_loss: 1.9554 - val_loss: 1.7434 - val_masked_acc: 0.6799 - val_masked_loss: 1.7434\n",
            "Epoch 6/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 151ms/step - loss: 1.6577 - masked_acc: 0.6933 - masked_loss: 1.6577 - val_loss: 1.5821 - val_masked_acc: 0.7086 - val_masked_loss: 1.5821\n",
            "Epoch 7/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 147ms/step - loss: 1.5414 - masked_acc: 0.7096 - masked_loss: 1.5414 - val_loss: 1.5006 - val_masked_acc: 0.7157 - val_masked_loss: 1.5006\n",
            "Epoch 8/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 150ms/step - loss: 1.4252 - masked_acc: 0.7258 - masked_loss: 1.4252 - val_loss: 1.3958 - val_masked_acc: 0.7283 - val_masked_loss: 1.3958\n",
            "Epoch 9/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 148ms/step - loss: 1.3543 - masked_acc: 0.7351 - masked_loss: 1.3543 - val_loss: 1.3035 - val_masked_acc: 0.7393 - val_masked_loss: 1.3035\n",
            "Epoch 10/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 150ms/step - loss: 1.2491 - masked_acc: 0.7461 - masked_loss: 1.2491 - val_loss: 1.2421 - val_masked_acc: 0.7512 - val_masked_loss: 1.2421\n",
            "Epoch 11/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 150ms/step - loss: 1.1032 - masked_acc: 0.7655 - masked_loss: 1.1032 - val_loss: 1.2064 - val_masked_acc: 0.7518 - val_masked_loss: 1.2064\n",
            "Epoch 12/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 1.0793 - masked_acc: 0.7686 - masked_loss: 1.0793"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 228ms/step - loss: 1.0793 - masked_acc: 0.7686 - masked_loss: 1.0793 - val_loss: 1.1615 - val_masked_acc: 0.7393 - val_masked_loss: 1.1615\n",
            "Epoch 13/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 152ms/step - loss: 1.0744 - masked_acc: 0.7686 - masked_loss: 1.0744 - val_loss: 1.1619 - val_masked_acc: 0.7588 - val_masked_loss: 1.1619\n",
            "Epoch 14/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 149ms/step - loss: 1.0416 - masked_acc: 0.7726 - masked_loss: 1.0416 - val_loss: 1.0925 - val_masked_acc: 0.7628 - val_masked_loss: 1.0925\n",
            "Epoch 15/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 151ms/step - loss: 0.9593 - masked_acc: 0.7854 - masked_loss: 0.9593 - val_loss: 1.1123 - val_masked_acc: 0.7668 - val_masked_loss: 1.1123\n",
            "Epoch 16/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 151ms/step - loss: 0.8780 - masked_acc: 0.7961 - masked_loss: 0.8780 - val_loss: 1.0528 - val_masked_acc: 0.7716 - val_masked_loss: 1.0528\n",
            "Epoch 17/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 153ms/step - loss: 0.8824 - masked_acc: 0.7945 - masked_loss: 0.8824 - val_loss: 1.0502 - val_masked_acc: 0.7750 - val_masked_loss: 1.0502\n",
            "Epoch 18/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 152ms/step - loss: 0.8712 - masked_acc: 0.7972 - masked_loss: 0.8712 - val_loss: 1.0246 - val_masked_acc: 0.7774 - val_masked_loss: 1.0246\n",
            "Epoch 19/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 152ms/step - loss: 0.8717 - masked_acc: 0.7973 - masked_loss: 0.8717 - val_loss: 1.0077 - val_masked_acc: 0.7831 - val_masked_loss: 1.0077\n",
            "Epoch 20/20\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 150ms/step - loss: 0.7691 - masked_acc: 0.8141 - masked_loss: 0.7691 - val_loss: 1.0101 - val_masked_acc: 0.7818 - val_masked_loss: 1.0101\n"
          ]
        }
      ],
      "source": [
        "# Train the translator (this takes some minutes so feel free to take a break)\n",
        "\n",
        "trained_translator, history = compile_and_train(translator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23b9301",
      "metadata": {
        "id": "d23b9301"
      },
      "source": [
        "\n",
        "## 4. Using the model for inference\n",
        "\n",
        "\n",
        "Now that our model is trained we can use it for inference. To help us with this the `generate_next_token` function is provided. Notice that this function is meant to be used inside a for-loop, so we feed to it the information of the previous step to generate the information of the next step. In particular, we need to keep track of the state of the pre-attention LSTM in the decoder and if we are done with the translation.\n",
        "\n",
        "Also notice that a `temperature` variable is introduced which determines how to select the next token given the predicted logits:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "522f6b6f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "522f6b6f"
      },
      "outputs": [],
      "source": [
        "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
        "    \"\"\"Generates the next token in the sequence\n",
        "\n",
        "    Args:\n",
        "        decoder (Decoder): The decoder\n",
        "        context (tf.Tensor): Encoded sentence to translate\n",
        "        next_token (tf.Tensor): The predicted next token\n",
        "        done (bool): True if the translation is complete\n",
        "        state (list[tf.Tensor, tf.Tensor]): Hidden states of the pre-attention LSTM layer\n",
        "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple(tf.Tensor, np.float, list[tf.Tensor, tf.Tensor], bool): The next token, log prob of said token, hidden state of LSTM and if translation is done\n",
        "    \"\"\"\n",
        "    # Get the logits and state from the decoder\n",
        "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
        "\n",
        "    # Trim the intermediate dimension\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    # If temp is 0 then next_token is the argmax of logits\n",
        "    if temperature == 0.0:\n",
        "        next_token = tf.argmax(logits, axis=-1)\n",
        "\n",
        "    # If temp is not 0 then next_token is sampled out of logits\n",
        "    else:\n",
        "        logits = logits / temperature\n",
        "        next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "    # Trim dimensions of size 1\n",
        "    logits = tf.squeeze(logits)\n",
        "    next_token = tf.squeeze(next_token)\n",
        "\n",
        "    # Get the logit of the selected next_token\n",
        "    logit = logits[next_token].numpy()\n",
        "\n",
        "    # Reshape to (1,1) since this is the expected shape for text encoded as TF tensors\n",
        "    next_token = tf.reshape(next_token, shape=(1,1))\n",
        "\n",
        "    # If next_token is End-of-Sentence token you are done\n",
        "    if next_token == eos_id:\n",
        "        done = True\n",
        "\n",
        "    return next_token, logit, state, done"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190d2d76",
      "metadata": {
        "id": "190d2d76"
      },
      "source": [
        "See how it works by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "9937547a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9937547a",
        "outputId": "ed0a6ab9-fcaa-4179-89e4-4d8a5fa4b931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token: [[4474]]\n",
            "Logit: -18.7384\n",
            "Done? False\n"
          ]
        }
      ],
      "source": [
        "# PROCESS SENTENCE TO TRANSLATE AND ENCODE\n",
        "\n",
        "# A sentence you wish to translate\n",
        "eng_sentence = \"I love languages\"\n",
        "\n",
        "# Convert it to a tensor\n",
        "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
        "\n",
        "# Vectorize it and pass it through the encoder\n",
        "context = english_vectorizer(texts).to_tensor()\n",
        "context = encoder(context)\n",
        "\n",
        "# SET STATE OF THE DECODER\n",
        "\n",
        "# Next token is Start-of-Sentence since you are starting fresh\n",
        "next_token = tf.fill((1,1), sos_id)\n",
        "\n",
        "# Hidden and Cell states of the LSTM can be mocked using uniform samples\n",
        "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
        "\n",
        "# You are not done until next token is EOS token\n",
        "done = False\n",
        "\n",
        "# Generate next token\n",
        "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
        "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170323dd",
      "metadata": {
        "id": "170323dd"
      },
      "source": [
        "\n",
        "##  5 - translation\n",
        "\n",
        "Now we can put everything together to translate a given sentence.we'll code the `translate` function below.\n",
        "\n",
        "This function will take care of the following steps:\n",
        "- Process the sentence to translate and encode it\n",
        "\n",
        "+ Set the initial state of the decoder\n",
        "\n",
        "- Get predictions of the next token (starting with the \\<SOS> token) for a maximum of iterations (in case the \\<EOS> token is never returned)\n",
        "    \n",
        "+ Return the translated text (as a string), the logit of the last iteration (this helps measure how certain was that the sequence was translated in its totality) and the translation in token format.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_text(tokens, id_to_word):\n",
        "    words = id_to_word(tokens)\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=\" \")\n",
        "    return result"
      ],
      "metadata": {
        "id": "pP05O1-WAXlx"
      },
      "id": "pP05O1-WAXlx",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "42c74f1f",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "42c74f1f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def translate(model, text, max_length=50, temperature=0.0):\n",
        "    \"\"\"Translate a given sentence from English to Portuguese\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): The trained translator\n",
        "        text (string): The sentence to translate\n",
        "        max_length (int, optional): The maximum length of the translation. Defaults to 50.\n",
        "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple(str, np.float, tf.Tensor): The translation, logit that predicted <EOS> token and the tokenized translation\n",
        "    \"\"\"\n",
        "    # Lists to save tokens and logits\n",
        "    tokens, logits = [], []\n",
        "\n",
        "    # PROCESS THE SENTENCE TO TRANSLATE\n",
        "\n",
        "    # Convert the original string into a tensor\n",
        "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
        "\n",
        "    # Vectorize the text using the correct vectorizer\n",
        "    context =  english_vectorizer(text).to_tensor()\n",
        "\n",
        "    # Get the encoded context (pass the context through the encoder)\n",
        "    # Hint: Remember you can get the encoder by using model.encoder\n",
        "    context = model.encoder(context)\n",
        "\n",
        "    # INITIAL STATE OF THE DECODER\n",
        "\n",
        "    # First token should be SOS token with shape (1,1)\n",
        "    next_token = tf.fill((1, 1), sos_id)\n",
        "\n",
        "    # Initial hidden and cell states should be tensors of zeros with shape (1, UNITS)\n",
        "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
        "\n",
        "    # You are done when you draw a EOS token as next token (initial state is False)\n",
        "    done = False\n",
        "\n",
        "    # Iterate for max_length iterations\n",
        "    for i in range(max_length):\n",
        "\n",
        "        # Generate the next token\n",
        "        next_token, logit, state, done = generate_next_token(\n",
        "            decoder=model.decoder,\n",
        "            context=context,\n",
        "            next_token=next_token,\n",
        "            done=done,\n",
        "            state=state,\n",
        "            temperature=temperature\n",
        "        )\n",
        "\n",
        "        # If done then break out of the loop\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Add next_token to the list of tokens\n",
        "        tokens.append(next_token)\n",
        "\n",
        "        # Add logit to the list of logits\n",
        "        logits.append(logit)\n",
        "\n",
        "    # Concatenate all tokens into a tensor\n",
        "    tokens = tf.concat(tokens, axis=-1)\n",
        "\n",
        "    # Convert the translated tokens into text\n",
        "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
        "    translation = translation.numpy().decode()\n",
        "\n",
        "    return translation, logits[-1], tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3525e8ba",
      "metadata": {
        "id": "3525e8ba"
      },
      "source": [
        "Try our function with temperature of 0, which will yield a deterministic output and is equivalent to a greedy decoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "daaea8c5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daaea8c5",
        "outputId": "38493800-b6ad-46fd-96fb-30ffe6190128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.0\n",
            "\n",
            "Original sentence: I love languages\n",
            "Translation: eu adoro idiomas .\n",
            "Translation tokens:[[  9 522 899   4]]\n",
            "Logit: -0.266\n"
          ]
        }
      ],
      "source": [
        "# Running this cell multiple times should return the same output since temp is 0\n",
        "\n",
        "temp = 0.0\n",
        "original_sentence = \"I love languages\"\n",
        "\n",
        "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
        "\n",
        "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d05129b",
      "metadata": {
        "id": "7d05129b"
      },
      "source": [
        "Try our function with temperature of 0.7 (stochastic output):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0e0697db",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e0697db",
        "outputId": "ab450c0a-f97f-4535-8147-7bacb0f8bab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.7\n",
            "\n",
            "Original sentence: I love languages\n",
            "Translation: eu adoro idiomas .\n",
            "Translation tokens:[[  9 522 899   4]]\n",
            "Logit: -0.380\n"
          ]
        }
      ],
      "source": [
        "# Running this cell multiple times should return different outputs since temp is not 0\n",
        "# You can try different temperatures\n",
        "\n",
        "temp = 0.7\n",
        "original_sentence = \"I love languages\"\n",
        "\n",
        "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
        "\n",
        "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba027524",
      "metadata": {
        "id": "ba027524"
      },
      "source": [
        "\n",
        "## 5. Minimum Bayes-Risk Decoding\n",
        "\n",
        "Getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
        "\n",
        "- Take several random samples\n",
        "+ Score each sample against all other samples\n",
        "- Select the one with the highest score\n",
        "\n",
        "We will be building helper functions for these steps in the following sections.\n",
        "\n",
        "With the ability to generate different translations by setting different temperature values and generate a bunch of translations and then determine which one is the best candidate. we will  do this by using the provided `generate_samples` function. This function will return any desired number of candidate translations alongside the log-probability for each one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "62301cd5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "62301cd5"
      },
      "outputs": [],
      "source": [
        "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
        "\n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    # Iterate for n_samples iterations\n",
        "    for _ in range(n_samples):\n",
        "\n",
        "        # Save the logit and the translated tensor\n",
        "        _, logp, sample = translate(model, text, temperature=temperature)\n",
        "\n",
        "        # Save the translated tensors\n",
        "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
        "\n",
        "        # Save the logits\n",
        "        log_probs.append(logp)\n",
        "\n",
        "    return samples, log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "06bd792c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06bd792c",
        "outputId": "c100ba8c-cfb5-4500-8c90-a3ef1428e2b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated tensor: [522, 1026, 4] has logit: -0.210\n",
            "Translated tensor: [9, 522, 899, 4] has logit: -0.444\n",
            "Translated tensor: [9, 522, 899, 4] has logit: -0.444\n",
            "Translated tensor: [9, 522, 899, 4] has logit: -0.444\n"
          ]
        }
      ],
      "source": [
        "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
        "\n",
        "for s, l in zip(samples, log_probs):\n",
        "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29b10677",
      "metadata": {
        "id": "29b10677"
      },
      "source": [
        "## Comparing overlaps\n",
        "\n",
        "Now that we can generate multiple translations it is time to come up with a method to measure the goodness of each one.One way to achieve this is by comparing each sample against the others.\n",
        "\n",
        "There are several metrics you can use for this purpose, will be calculating scores for **unigram overlaps**.\n",
        "\n",
        "One of these metrics is the widely used yet simple [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) which gets the intersection over union of two sets. The `jaccard_similarity` function returns this metric for any pair of candidate and reference translations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "edb54a71",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "edb54a71"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "\n",
        "    # Convert the lists to sets to get the unique tokens\n",
        "    candidate_set = set(candidate)\n",
        "    reference_set = set(reference)\n",
        "\n",
        "    # Get the set of tokens common to both candidate and reference\n",
        "    common_tokens = candidate_set.intersection(reference_set)\n",
        "\n",
        "    # Get the set of all tokens found in either candidate or reference\n",
        "    all_tokens = candidate_set.union(reference_set)\n",
        "\n",
        "    # Compute the percentage of overlap (divide the number of common tokens by the number of all tokens)\n",
        "    overlap = len(common_tokens) / len(all_tokens)\n",
        "\n",
        "    return overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "fc3384bf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc3384bf",
        "outputId": "5f1beb47-b3a9-462f-9e20-396cd0ce61c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n"
          ]
        }
      ],
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 3, 4]\n",
        "\n",
        "js = jaccard_similarity(l1, l2)\n",
        "\n",
        "print(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2510e3d",
      "metadata": {
        "id": "b2510e3d"
      },
      "source": [
        "\n",
        "## 6 - rouge1_similarity\n",
        "\n",
        "Jaccard similarity is good but a more commonly used metric in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1, we can output the scores for both precision and recall when comparing two samples. To get the final score, we will want to compute the F1-score as given by:\n",
        "\n",
        "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n",
        "\n",
        "For the implementation of the `rouge1_similarity` function you want to use the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) class from the Python standard library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "fb2e0a00",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "fb2e0a00"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rouge1_similarity(candidate, reference):\n",
        "    \"\"\"Computes the ROUGE 1 score between two token lists\n",
        "\n",
        "    Args:\n",
        "        candidate (list[int]): Tokenized candidate translation\n",
        "        reference (list[int]): Tokenized reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: Overlap between the two token lists\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a frequency table of the candidate and reference tokens\n",
        "    candidate_word_counts = Counter(candidate)\n",
        "    reference_word_counts = Counter(reference)\n",
        "\n",
        "    # Initialize overlap at 0\n",
        "    overlap = 0\n",
        "\n",
        "    # Iterate over the tokens in the candidate frequency table\n",
        "\n",
        "    for token in candidate_word_counts.keys():\n",
        "\n",
        "        # Get the count of the current token in the candidate frequency table\n",
        "        token_count_candidate = candidate_word_counts.get(token,0)\n",
        "\n",
        "        # Get the count of the current token in the reference frequency table\n",
        "        token_count_reference = reference_word_counts.get(token,0)\n",
        "\n",
        "        # Update the overlap by getting the minimum between the two token counts above\n",
        "        overlap += min(token_count_candidate,token_count_reference)\n",
        "\n",
        "    # Compute the precision\n",
        "\n",
        "    precision = overlap/(sum(candidate_word_counts.values()))\n",
        "\n",
        "    # Compute the recall\n",
        "\n",
        "    recall = overlap/(sum(reference_word_counts.values()))\n",
        "\n",
        "    if precision + recall != 0:\n",
        "        # Compute the Rouge1 Score\n",
        "\n",
        "        f1_score = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        return f1_score\n",
        "\n",
        "\n",
        "    return 0 # If precision + recall = 0 then return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "14bb5295",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14bb5295",
        "outputId": "5f01ced5-73a3-426e-d8eb-01d6c30d88e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n"
          ]
        }
      ],
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 3, 4]\n",
        "\n",
        "r1s = rouge1_similarity(l1, l2)\n",
        "\n",
        "print(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf8a058",
      "metadata": {
        "id": "aaf8a058"
      },
      "source": [
        "## Computing the Overall Score\n",
        "\n",
        "\n",
        "We will now build a function to generate the overall score for a particular sample. We need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 through 30. Then, we compare sentence 2 to sentences 1 and 3 through 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
        "\n",
        "- Get similarity score between sample 1 and sample 2\n",
        "+ Get similarity score between sample 1 and sample 3\n",
        "- Get similarity score between sample 1 and sample 4\n",
        "+ Get average score of the first 3 steps. This will be the overall score of sample 1\n",
        "- Iterate and repeat until samples 1 to 4 have overall scores.\n",
        "\n",
        "\n",
        "The results will be stored in a dictionary for easy lookups.\n",
        "\n",
        "\n",
        "##  7 - average_overlap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "142264ff",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "142264ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "def average_overlap(samples, similarity_fn):\n",
        "    \"\"\"Computes the arithmetic mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        samples (list[list[int]]): Tokenized version of translated sentences\n",
        "        similarity_fn (Function): Similarity function used to compute the overlap\n",
        "\n",
        "    Returns:\n",
        "        dict[int, float]: A dictionary mapping the index of each translation to its score\n",
        "    \"\"\"\n",
        "    # Initialize dictionary\n",
        "    scores = {}\n",
        "\n",
        "    # Iterate through all samples (enumerate helps keep track of indexes)\n",
        "    for index_candidate, candidate in enumerate(samples):\n",
        "\n",
        "\n",
        "        # Initially overlap is zero\n",
        "        overlap = 0\n",
        "\n",
        "        # Iterate through all samples (enumerate helps keep track of indexes)\n",
        "        for index_sample, sample in enumerate(samples):\n",
        "\n",
        "            # Skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "\n",
        "            # Get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "\n",
        "            # Add the sample overlap to the total overlap\n",
        "            overlap += sample_overlap\n",
        "\n",
        "\n",
        "        # Get the score for the candidate by computing the average\n",
        "        score = overlap / (len(samples) - 1)\n",
        "\n",
        "        # Only use 3 decimal points\n",
        "        score = round(score, 3)\n",
        "\n",
        "        # Save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "f36cf403",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f36cf403",
        "outputId": "54c84596-3aaf-4a16-cae5-1e43c3df4c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n",
            "\n",
            "{0: 0.45, 1: 0.625, 2: 0.575}\n"
          ]
        }
      ],
      "source": [
        "# Test with Jaccard similarity\n",
        "\n",
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 4]\n",
        "l3 = [1, 2, 4, 5]\n",
        "\n",
        "avg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n",
        "\n",
        "print(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "d961a304-7c03-4ecb-ba5f-c8747ed3ec39",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d961a304-7c03-4ecb-ba5f-c8747ed3ec39",
        "outputId": "489ea6c3-d49b-4284-e58f-1ac40ee34774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average overlap between lists: [1, 2, 3], [1, 4], [1, 2, 4, 5] and [5, 6] using Rouge1 similarity is:\n",
            "\n",
            "{0: 0.324, 1: 0.356, 2: 0.524, 3: 0.111}\n"
          ]
        }
      ],
      "source": [
        "# Test with Rouge1 similarity\n",
        "\n",
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 4]\n",
        "l3 = [1, 2, 4, 5]\n",
        "l4 = [5,6]\n",
        "\n",
        "avg_ovlp = average_overlap([l1, l2, l3, l4], rouge1_similarity)\n",
        "\n",
        "print(f\"average overlap between lists: {l1}, {l2}, {l3} and {l4} using Rouge1 similarity is:\\n\\n{avg_ovlp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4482249",
      "metadata": {
        "id": "e4482249"
      },
      "source": [
        "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. This is implemented in the `weighted_avg_overlap` function below and you can use it in your experiments to see which one will give better results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "398714be",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "398714be"
      },
      "outputs": [],
      "source": [
        "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
        "\n",
        "    # Scores dictionary\n",
        "    scores = {}\n",
        "\n",
        "    # Iterate over the samples\n",
        "    for index_candidate, candidate in enumerate(samples):\n",
        "\n",
        "        # Initialize overlap and weighted sum\n",
        "        overlap, weight_sum = 0.0, 0.0\n",
        "\n",
        "        # Iterate over all samples and log probabilities\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "\n",
        "            # Skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "\n",
        "            # Convert log probability to linear scale\n",
        "            sample_p = float(np.exp(logp))\n",
        "\n",
        "            # Update the weighted sum\n",
        "            weight_sum += sample_p\n",
        "\n",
        "            # Get the unigram overlap between candidate and sample\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "\n",
        "            # Update the overlap\n",
        "            overlap += sample_p * sample_overlap\n",
        "\n",
        "        # Compute the score for the candidate\n",
        "        score = overlap / weight_sum\n",
        "\n",
        "        # Only use 3 decimal points\n",
        "        score = round(score, 3)\n",
        "\n",
        "        # Save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "e3dfd6d3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3dfd6d3",
        "outputId": "e766368e-d56e-4ab3-a989-25a79a422b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weighted average overlap using Jaccard similarity is:\n",
            "\n",
            "{0: 0.443, 1: 0.631, 2: 0.558}\n"
          ]
        }
      ],
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 4]\n",
        "l3 = [1, 2, 4, 5]\n",
        "log_probs = [0.4, 0.2, 0.5]\n",
        "\n",
        "w_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n",
        "\n",
        "print(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb0b4db",
      "metadata": {
        "id": "cdb0b4db"
      },
      "source": [
        "## mbr_decode\n",
        "\n",
        "We will now put everything together in the the `mbr_decode` function below. This final st is just a wrapper around all the cool stuff we have coded so far!\n",
        "\n",
        "we can use it to play around, trying different numbers of samples, temperatures and similarity functions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "6fcfa640",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "6fcfa640"
      },
      "outputs": [],
      "source": [
        "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
        "\n",
        "    # Generate samples\n",
        "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
        "\n",
        "    # Compute the overlap scores\n",
        "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
        "\n",
        "    # Decode samples\n",
        "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
        "\n",
        "    # Find the key with the highest score\n",
        "    max_score_key = max(scores, key=lambda k: scores[k])\n",
        "\n",
        "    # Get the translation\n",
        "    translation = decoded_translations[max_score_key]\n",
        "\n",
        "    return translation, decoded_translations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Your Own Sentence !"
      ],
      "metadata": {
        "id": "A6xfuwX49CTL"
      },
      "id": "A6xfuwX49CTL"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "99507fcc-7727-45e7-933b-d3a08034f731",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99507fcc-7727-45e7-933b-d3a08034f731",
        "outputId": "5a914b7d-e9c6-4a69-8be3-3f249316c1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation candidates:\n",
            "eu adoro idiomas .\n",
            "eu adoro linguas .\n",
            "eu adoro idiomas .\n",
            "eu adoro linguas .\n",
            "eu adoro idiomas .\n",
            "eu adoro idiomas .\n",
            "eu adoro linguas .\n",
            "eu adoro idiomas .\n",
            "eu adoro idiomas .\n",
            "adoro linguas .\n",
            "\n",
            "Selected translation: eu adoro idiomas .\n"
          ]
        }
      ],
      "source": [
        "english_sentence = \"I love languages\"\n",
        "\n",
        "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
        "\n",
        "print(\"Translation candidates:\")\n",
        "for c in candidates:\n",
        "    print(c)\n",
        "\n",
        "print(f\"\\nSelected translation: {translation}\")"
      ]
    }
  ],
  "metadata": {
    "grader_version": "1",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}